\documentclass[sigconf]{acmart}
\settopmatter{printacmref=false, printccs=false, printfolios=true}
\renewcommand\footnotetextcopyrightpermission[1]{}


\usepackage{booktabs} % For formal tables


%%%%%
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}

\begin{document}
\title{Concurrency and Parallelism Project}


\author{Guilherme Fernandes}
\email{60045}

\author{Vladyslav Mikytiv}
\email{60735}


\begin{abstract}
This report provides the explanation and the implementation of parallelisation of the algorithm of equalization of images. Both an OpenMP and CUDA implementations.

\end{abstract}

% We no longer use \terms command
%\terms{Theory}

\keywords{OpenMP Parallelism C++ CUDA}


\maketitle

\section{Critical Code Analysis and Parallelization}
\subsection{The first steps}

At first we ran the code without doing any modifications and used the profiler in order to identify the hot spots doing a average value in each image of the time lost in functions from the profiler. The algorithm runs for 100 iterations.

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|}
\hline
\textbf{Function} & \textbf{Computation Time Lost (\%)} \\
\hline
correctColor\&rescale & 57.98\% \\
\hline
grayScale & 23.17\% \\
\hline
normalize & 18.24\% \\
\hline
\end{tabular}
\vspace{0.2cm} % Adjust the space here
\caption{Average Computation Time Lost in Functions}
\label{table:computation_time_lost}
\end{table}
As seen in Listing \ref{table:computation_time_lost} the zones of the code that took a long time to perform the computation and in overall slow down the algorithm are:
the \textbf{\texttt{normalization}} of the image, the \textbf{\texttt{correction of color}}, the \textbf{\texttt{convertion to the greyscale}} and some other
not so computational heavy functions.


 To reduce the impact on processing time caused by these functions within the algorithm, we integrated OpenMP directives into these specific parts of the code to accelerate computation.
 
 It's important to highlight the \textbf{thread management}. Since we are working with images we will have to do some simples calculus to determine the amount of work that
 each thread will be given. For that we must perform two calculations (one for RGB and one for the grey scale). For the RGB  we will calculate $\texttt{WIDTH * HEIGHT * CHAN\_SIZE}$  (the 3 RGB channels) and to obtain the \texttt{CHUNK\_SIZE\_RGB} we just divide $\texttt{WIDTH * HEIGHT * CHAN\_SIZE}$  by \texttt{N\_THREADS}. For the grey scale it's similar but we don't multiply by \texttt{CHAN\_SIZE} getting the \texttt{CHUNK\_SIZE}. Now we will have the work that will be balanced between threads.
 
\subsection{\textbf{Function parallelization}}

The \textbf{normalization} function will be improved with the following code: \texttt{\textbf{pragma omp parallel for}} with the option \texttt{\textbf{schedule(static, chunk\_size\_channels)}} and \texttt{\textbf{num\_threads(n\_threads)}}. This code optimization can be seen in Listing \ref{lst:normalization_function}.

\begin{lstlisting}[language=C, caption=Normalization Function, label={lst:normalization_function}]
void normalize(//omitting for space) {
    #pragma omp parallel for schedule(static, chunk_size_channels) num_threads(n_threads)
    for (int i = 0; i < size_channels; i++)
        uchar_image[i] = (unsigned char) (255 * input_image_data[i]);
}
\end{lstlisting}
The \textbf{grey scale conversion} will be improved in two ways. We will mix the \texttt{fill\_histogram}
function with this one in order to do everything in the same function. Besides that we also apply the following OpenMP directives: \texttt{\textbf{pragma omp parallel for}}
with the option \texttt{\textbf{reduction(+:histogram)}} and \texttt{\textbf{num\_threads(n\_threads)}}. This code optimization can be seen in Listing \ref{lst:gray_function}.
\begin{lstlisting}[language=C, caption=Grey Conversion Function, label={lst:gray_function}]
	void extractGrayScale(//omitting for space) {
       	// filling the histogram with zeroes
        #pragma omp parallel for reduction(+:histogram) num_threads(n_threads)
        for (int i = 0; i < size; i++){
                auto r = uchar_image[num_chan * i];
                auto g = uchar_image[num_chan * i + 1];
                auto b = uchar_image[num_chan * i + 2];
                gray_image[i] = // Calculated
                histogram[gray_image[i]]++;
            }
    }
\end{lstlisting}
We can't parallelize the \textbf{CDF calculcation} because it has true dependencies between iterations and also it's not worth due to the number of iterations (256).
Another function that we managed to simplify was the \texttt{cdf\_min\_loop}. Since we minimum will always be on the first position we just return it and that's how we compute
the minimum of the CDF.

The \texttt{\textbf{correct\_color\_loop}} can be mixed with the \texttt{\textbf{rescale}} following the same strategy as we did with \texttt{grey scale} and \texttt{fill historgram} functions. The code optimization is the same as Listing \ref{lst:normalization_function} and can be seen in Listing \ref{lst:correct_function}.
\begin{lstlisting}[language=C, caption=Color Correction Function, label={lst:correct_function}]
	void correct_color_loop_and_rescale(//omitting for space) {
        #pragma omp parallel for schedule(static, chunk_size_channels) num_threads(n_threads)
        for (int i = 0; i < size_channels; i++)
        {
            uchar_image[i] = correct_color(cdf[uchar_image[i]], cdf_min);
            output_image_data[i] = static_cast<float>(uchar_image[i]) / 255.0f;
        }
    }
\end{lstlisting}
\subsection{Decions}
It's important to justify our reasoning. We used \texttt{\textbf{static}} in every single call of the OpenMP directives because the extra overhead of orchistrating the threads with a certain amount of work did not compensate. As we can see in the Table \ref{table:staticanddynamic} \texttt{\textbf{static}} always shows better results and it's coherent to the theoretical analysis. For this simulation we used 16 threads and 100 iterations of the algorithm.

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Dynamic Speedup} & \textbf{Static Speedup} & \textbf{Image Name} \\
\hline
9.7 & 11.5& borabora.ppm \\
\hline
9.7 & 11.8 & input01.ppm \\
\hline
7.7 & 9.9 & sample.ppm \\
\hline
\end{tabular}
\vspace{0.2cm} % Adjust the space here
\caption{Dynamic vs Static Mean Execution Times}
\label{table:staticanddynamic}
\end{table}

\section{Metric Analysis - OpenMP}
Now, let's assess the impact of these changes on the runtime of our program. For that we will use \textbf{speedup} and \textbf{efficienty} and calculate the mean values of 5 runs for each thread.

The code was in a cluster executed with the following specifications: \textbf{2x Intel Xeon E5-2609 v4}, with \textbf{16 cores} and \textbf{NVIDIA Quadro M2000} GPU.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{images/openMP.png}
  \caption{Speedup across different number of threads usage of different images}
 \label{fig1}
\end{figure}

Figure \ref{fig1} describes the speed up for the different images in our test set and Table \ref{table:standard_deviations} the standard deviations in the execution time. We can clearly see a rise on the speed up until a certain point. After that point the value of the speed up begins to decrease. The optimal \texttt{NR\_THREADS} to perform the computation is 16.

In order to calculate the efficiency we will use the best result that we had: the one with 16 threads. For that we define $S_{16}$ which is the speed up with 16 threads and it will be equal to $$S_{16} \approx 11$$

And the efficiency of the parallelel implementation in Section 1 get's us an efficiency value of: $$E_{16} = \frac{S_{16}}{16} \approx 69\%$$

\begin{table}[h!]
\centering
\begin{tabular}{cccc}
\toprule
\textbf{NÂº of Threads} & \textbf{borabora.ppm} & \textbf{input01.ppm} & \textbf{sample.ppm} \\
\midrule
1  & 21 & 18 & 18  \\
2  & 16 & 21 & 59  \\
3  & 9  & 10 & 249 \\
4  & 3  & 15 & 230 \\
5  & 19 & 14 & 403 \\
6  & 15 & 15 & 77  \\
7  & 28 & 4  & 98  \\
8  & 16 & 3  & 313 \\
9  & 3  & 9  & 874 \\
10 & 20 & 9  & 452 \\
11 & 5  & 10 & 58  \\
12 & 2  & 8  & 251 \\
13 & 1  & 9  & 84  \\
14 & 3  & 6  & 59  \\
15 & 5  & 7  & 37  \\
16 & 3  & 11 & 46  \\
17 & 4  & 5  & 47  \\
18 & 3  & 2  & 48  \\
19 & 4  & 2  & 128 \\
20 & 2  & 1  & 70  \\
\bottomrule
\end{tabular}
\caption{Standard Deviations of Execution Times}
\label{table:standard_deviations}
\end{table}

\section{CUDA}
We've attained speed enhancements leveraging the OpenMP library for parallelizing our code. However, there exists an avenue to further escalate our computational efficiency. By harnessing the GPU architecture through CUDA, we can unlock additional acceleration potential.

To leverage the GPU effectively, we need to define kernels for specific regions of our code that we wish to offload to the GPU for computation. This necessitates defining grids use in CUDA kernels.

Utilizing the GPU for optimization involves creating kernels to transfer data to the GPU, perform computations, and retrieve the results back to the CPU. Given the high cost associated with these operations, minimizing the number of kernels is imperative.

The initial kernel handles both the normalization and grayscale operations. We can merge these two operations in one kernel since we don't need any prior computation before to calculate it. We don't need to create an extra overhead with two kernels in here. The Listing \ref{lst:gray_norm_function} shows this kernel implementation.

\begin{lstlisting}[language=C, caption=Grey Conversion and Normalization, label={lst:gray_norm_function}]
__global__ void normalize_kernel(//omitting for space) {
        int ii = blockIdx.y * blockDim.y + threadIdx.y;
        int jj = blockIdx.x * blockDim.x + threadIdx.x;
        int idx = (ii * width + jj)*3;
        if (ii < height && jj < width) {
            for(int i = 0; i < num_chan; i++) 
                // Normalize logic
            auto r = uchar_image[idx];
            auto g = uchar_image[idx + 1];
            auto b = uchar_image[idx + 2];
            idx =ii * width + jj;
            gray_image[idx] = // Calculation
        }
    }
\end{lstlisting}

Before proceeding to the second kernel, it's essential to calculate the histogram. While we could create a custom kernel for this task, a more efficient approach involves leveraging the \texttt{\textbf{CUB}} library ~\cite{cub}. By utilizing its operations, we can significantly enhance the performance of our code.

Using \texttt{\textbf{cub::DeviceHistogram::HistogramEven}} we are able to calculate the histogram in the GPU without making the kernel ourselves.

After having the histogram we can calculate the probabilities in order to use them in another call to the \texttt{\textbf{CUB}} library. For that we create a second kernel, which will seem more clear why ahead. This one will differ from the first one in the \texttt{\textbf{numBlocks}} and \texttt{\textbf{blockSize}}. The \texttt{\textbf{numBlocks}} will be $256$ and the \texttt{\textbf{blockSize}} will be \texttt{(HISTOGRAM\_LEN+ blockSize-1) / blockSize}, that's why we couldn't re-use the first kernel due to this size and the \texttt{CUB} library usage.

Once we have computed this probability array, we can further utilize it in another function provided by the \texttt{\textbf{CUB}}  library.

The \texttt{\textbf{cub::DeviceScan::InclusiveSum}} function facilitates the calculation of the final cumulative distribution function (CDF) values. This not only streamlines the programming process but also optimizes computation time, offering significant efficiency gains. We used the \texttt{\textbf{InclusiveSum}} because it was not possible to use \texttt{\textbf{cub::DeviceScan::InclusiveScan}} due to restrictions in the library (there was no way to pass the probabilities to this call). That's why we must calculate the probabilities beforehand and use the \texttt{\textbf{InclusiveSum}}.

Ultimately, we employ our final kernel to run the last part of the algorithm. The code can be seen in Listing \ref{lst:last_func}.

\begin{lstlisting}[language=C, caption=Correct and Rescale, label={lst:last_func}]
 __global__ void correct_kernel(//omitting for space) {
        int ii = blockIdx.y * blockDim.y + threadIdx.y;
        int jj = blockIdx.x * blockDim.x + threadIdx.x;
        int idx = (ii * width + jj)*3;

        if (ii < height && jj < width) {
            for (int i = 0; i < num_chan; i++) {
                auto cdf_val = d_cdf[uchar_image[idx+i]];
                float cdf_min = d_cdf[0];
                
                uchar_image[idx+i] = // Logic

                output_image_data[idx+i] = // Logic
            }
        }
    }
\end{lstlisting}
These kernels constitute the entirety of the GPU-accelerated operations. Due to space constraints, significant portions of the code have been omitted. For the complete implementation, please refer to the accessible code at \url{https://github.com/Gui28F/CP-Project}. 

\section{Metric Analysis - CUDA}
Utilizing the capabilities of the GPU necessitates defining the grid dimensions. To determine the optimal \texttt{TILE\_WIDTH} value, we conducted an experiment, the results of which are depicted in Figure \ref{fig2}. It was discerned that the most favorable value was 16. Notably, we are constrained from exceeding 32 due to the limitation imposed by the maximum number of thread per block, capped at $32 * 32 = 1024$.

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{images/tilesss.png}
  \caption{Tiles Width and Execution Time}
 \label{fig2}
\end{figure}
To effectively evaluate the performance of the GPU-accelerated version, it's important to compare it against the most optimized configuration achieved with OpenMP (specifically, the one utilizing 16 threads). Figure \ref{fig3} illustrates the differences in speedup compared to the optimal OpenMP configuration (and the others also).

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{images/openMPCUDA.png}
  \caption{OpenMP vs CUDA}
 \label{fig3}
\end{figure}




\section{Final Results}
After implementing optimizations using \textbf{OpenMP} and \textbf{CUDA}, we now have a clearer understanding of the overall performance. The table presents execution times for the purely sequential implementation, the \textbf{OpenMP} implementation, and the \textbf{CUDA} implementation. These results, obtained from 100 iterations of the algorithm, represent the mean calculated values derived from running the algorithm five times. The names were shortened in Table \ref{table:final}\footnote{\textbf{Description:} SQT -- Sequential Time, OMPT -- OpenMP Time, CUDAT -- CUDA Time}.

Each generation of results was derived from the average of five separate runs of algorithm with 100 iterations. By calculating the mean of these five separate executions, we aimed to obtain more reliable and trustworthy results. This approach ensured that our findings were not skewed by anomalies in any single run, providing a more accurate representation of the algorithm's performance.

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{SQT (ms)} & \textbf{OMPT (ms)} & \textbf{CUDAT (ms)} & \textbf{Image} \\
\hline
1140 & 98 & 76 & borabora.ppm \\
\hline
956 & 76 & 63 & input01.ppm \\
\hline
43593 & 4371 & 2635 & sample.ppm \\
\hline
\end{tabular}
\vspace{0.2cm} % Adjust the space here
\caption{Final Execution Times}
\label{table:final}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{OpenMP Speedup} & \textbf{CUDA Speedup} & \textbf{Image} \\
\hline
11.5 & 15 & borabora.ppm \\
\hline
11.8 & 15.2 & input01.ppm \\
\hline
9.9 & 16.5 & sample.ppm \\
\hline
\end{tabular}
\vspace{0.2cm} % Adjust the space here
\caption{Final Speedup Values}
\label{table:speeds}
\end{table}

As observed in all the images of Table \ref{table:final} and Table \ref{table:speeds} , even with the best OpenMP configuration, CUDA consistently outperforms it in terms of speed. The superior capabilities of the GPU significantly accelerate our computations, and this performance difference becomes increasingly noticeable with larger image sizes.

\section{Tests}
During development, we utilized GoogleTest to guide our implementations for both OpenMP and CUDA. These tests ensured the accuracy of the pixel values in the resulting images, verifying correctness beyond mere visual inspection. Additionally, the test files used are available in our GitHub repository, providing transparency and allowing others to validate our results.

\section{Conclusions}
Whether through the integration of OpenMP directives or harnessing the power of GPU acceleration, we consistently observe improved results compared to the raw sequential implementation. In all our tests, CUDA consistently delivers superior performance compared to OpenMP. The enhanced capabilities of GPUs allow CUDA to handle computations more efficiently, resulting in faster processing times across various image sizes and complexities. This makes CUDA the preferred choice for achieving optimal performance in our implementations.

\section{Code}
All the code mentioned above can be consulted in \url{https://github.com/Gui28F/CP-Project}.
\section{Individual Contribution and Acknowledgments}
The project was a joint effort, with both team members sharing the workload equally. We utilized pair programming sessions on Discord calls and in the lab to accomplish our tasks effectively. We are grateful to Professor HervÃ© for his invaluable support and constant availability to assist us throughout the project.
\bibliographystyle{acm}
\bibliography{my_bib}

\end{document}